{"cells":[{"cell_type":"markdown","source":["# **Imports and required installation**"],"metadata":{"id":"E9Qo8OXOIDx8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XrWjrWAquZy_"},"outputs":[],"source":["!pip install pytorch-lightning wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhIv3sZP0k0J"},"outputs":[],"source":["import os\n","import zipfile\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","\n","import torchvision.models as models\n","from torchvision import transforms\n","\n","import torchmetrics\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning.loggers import WandbLogger\n","\n","import wandb\n","\n"]},{"cell_type":"code","source":["wandb.login()\n"],"metadata":{"id":"XB1EEfRrdN0O"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tnh0os32lLDk"},"outputs":[],"source":["input_size = (400,400)\n","number_of_classes = 5\n","root_dir_for_dataset = '/tmp/dino'\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","source":["# **Dataset preparation**"],"metadata":{"id":"BIVrXVypIQLL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HWL3VyJsp9TB"},"outputs":[],"source":["image_zip = '/content/drive/MyDrive/dino.zip'\n","zip_ref   = zipfile.ZipFile(image_zip, 'r')\n","zip_ref.extractall('/tmp/')\n","zip_ref.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o-gu_uO3my43"},"outputs":[],"source":["class My_Dataset(Dataset):\n","  def __init__(self,root_dir,transforms = None):\n","    self.root_dir = root_dir\n","    self.classes = os.listdir(self.root_dir)\n","    self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n","    self.image_files, self.labels = self.load_image_files()\n","    self.transforms = transforms\n","\n","  def __len__(self):\n","    return len(self.image_files)\n","\n","  def __getitem__(self, idx):\n","      image_path = self.image_files[idx]\n","      label = self.labels[idx]\n","\n","      image = Image.open(image_path).convert(\"RGB\")\n","      if self.transforms:\n","           image = self.transforms(image)\n","\n","      return image, label\n","\n","  def load_image_files(self):\n","        image_files = []\n","        labels = []\n","        for class_name in self.classes:\n","            class_dir = os.path.join(self.root_dir, class_name)\n","            if os.path.isdir(class_dir):\n","                images = os.listdir(class_dir)\n","                for image_name in images:\n","                    image_path = os.path.join(class_dir, image_name)\n","                    image_files.append(image_path)\n","                    labels.append(self.class_to_idx[class_name])\n","        return image_files, labels\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4lp6jLXbyfSK"},"outputs":[],"source":["class DataModule(pl.LightningDataModule):\n","\n","  def __init__(self,root_dir,batch_size):\n","    super(DataModule,self).__init__()\n","    self.root_dir= root_dir\n","    self.batch_size = batch_size\n","    self.transform = transforms.Compose([\n","    transforms.Resize(input_size),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.4131, 0.3576, 0.2830], std=[0.2309, 0.2194, 0.2037])\n","                ])\n","\n","  def setup(self,stage=None):\n","    dataset = My_Dataset(root_dir=root_dir_for_dataset, transforms=self.transform)\n","    self.train_set, self.val_set ,self.test_set = torch.utils.data.random_split(dataset,[1700,423,150])\n","\n","  def train_dataloader(self):\n","      return torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True,drop_last=True,num_workers=7)\n","\n","  def test_dataloader(self):\n","      return torch.utils.data.DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False,drop_last=True,num_workers=7)\n","\n","  def val_dataloader(self):\n","      return torch.utils.data.DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False,drop_last=True,num_workers=7)\n","\n","\n"]},{"cell_type":"markdown","source":["# **Model**"],"metadata":{"id":"kz-w1h0jIZl9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"T4NIu7f3u3hR"},"outputs":[],"source":["class Model(pl.LightningModule):\n","\n","    def __init__(self, model,input_shape, num_classes, learning_rate, transfer=True):\n","        super(Model,self).__init__()\n","\n","        # log hyperparameters\n","\n","        self.save_hyperparameters()\n","        self.learning_rate = learning_rate\n","        self.dim = input_shape\n","        self.num_classes = num_classes\n","        self.pretrained_model = model\n","        self.feature_extractor = getattr(models, self.pretrained_model)(pretrained=transfer)\n","\n","\n","        if transfer:\n","            # layers are frozen by using eval()\n","            self.feature_extractor.eval()\n","            # freeze params\n","            for param in self.feature_extractor.parameters():\n","                param.requires_grad = False\n","\n","        n_sizes = self._get_conv_output(input_shape)\n","\n","\n","        self.classifier = nn.Linear(n_sizes, num_classes)\n","\n","\n","        self.loss_function = nn.CrossEntropyLoss()\n","        self.accuracy =  torchmetrics.Accuracy(task=\"multiclass\",num_classes=self.num_classes)\n","\n","    # returns the size of the output tensor going into the Linear layer from the conv block.\n","    def _get_conv_output(self, shape):\n","        batch_size = 1\n","        tmp_input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n","\n","\n","        output_feat = self._forward_features(tmp_input)\n","        n_size = output_feat.data.view(batch_size, -1).size(1)\n","        return n_size\n","\n","    # returns the feature tensor from the conv block\n","    def _forward_features(self, x):\n","        x = self.feature_extractor(x)\n","        return x\n","\n","    # will be used during inference\n","    def forward(self, x):\n","       x = self._forward_features(x)\n","       x = x.view(x.size(0), -1)\n","       x = self.classifier(x)\n","\n","       return x\n","\n","    def training_step(self,batch,batch_idx):\n","        loss, scores, labels = self._common_step(batch,batch_idx)\n","        accuracy = self.accuracy(scores,labels)\n","        self.log_dict({\"train_loss\": loss , \"train_acc\": accuracy}, prog_bar=True,on_epoch=True)\n","        return loss\n","\n","    def test_step(self,batch,batch_idx):\n","        loss, scores, labels = self._common_step(batch,batch_idx)\n","        accuracy = self.accuracy(scores,labels)\n","        self.log_dict({\"test_loss\": loss , \"test_acc\": accuracy}, prog_bar=True,on_step=False,on_epoch=True)\n","        return loss\n","\n","    def validation_step(self,batch,batch_idx):\n","        loss, scores, labels = self._common_step(batch,batch_idx)\n","        accuracy = self.accuracy(scores,labels)\n","        self.log_dict({\"val_loss\": loss , \"val_acc\": accuracy}, prog_bar=True,on_step=False,on_epoch=True)\n","        return loss\n","\n","    def _common_step(self,batch,batch_idx):\n","        inputs,labels = batch\n","        scores = self.forward(inputs)\n","        loss = self.loss_function(scores,labels)\n","        return loss, scores, labels\n","\n","    def configure_optimizers(self) :\n","        return optim.Adam(params=self.parameters(),lr=self.learning_rate)\n"]},{"cell_type":"markdown","source":["# **Finding best parameters**"],"metadata":{"id":"ioJmSUSVId3Q"}},{"cell_type":"code","source":["sweep_config = {'method': 'random',\n"," 'metric': {'goal': 'maximize', 'name': 'test_acc'},\n"," 'parameters': {'batch_size': {'values': [4,8,16]\n","                               },\n","\n","                'learning_rate': {'distribution': 'log_uniform_values',\n","                                  'max': 1e-2,\n","                                  'min': 1e-5\n","                                  },\n","\n","                'models': {'values': ['resnet18','vgg16','squeezenet1_0','inception_v3','googlenet','mobilenet_v2','densenet161']}\n","                }\n","                }\n","\n","sweep_id = wandb.sweep(sweep_config, project=\"classification-pytorch-lightning\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EmreNtOLdTnz","executionInfo":{"status":"ok","timestamp":1700316813776,"user_tz":-60,"elapsed":1255,"user":{"displayName":"miro kery","userId":"14353713734412504608"}},"outputId":"7057eee9-080c-47cc-b4e2-9a5491fa7ebc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Create sweep with ID: dtmrenq3\n","Sweep URL: https://wandb.ai/mirokery/classification-pytorch-lightning/sweeps/dtmrenq3\n"]}]},{"cell_type":"markdown","source":["# **Training models**"],"metadata":{"id":"fIm9zRuwIklA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uI22vNhlKBQK"},"outputs":[],"source":["def train(config=None):\n","    # Initialize a new wandb run\n","    with wandb.init(config=config,project=\"classification-pytorch-lightning\",name=\"training_with_sweeps_models\"):\n","        # If called by wandb.agent, as below,\n","        # this config will be set by Sweep Controller\n","        config = wandb.config\n","        pretrained_model = wandb.config.models\n","        batch_size=wandb.config.batch_size\n","        lr=  wandb.config.learning_rate\n","        dm = DataModule(root_dir=root_dir_for_dataset,batch_size=batch_size)\n","        dm.setup()\n","\n","        model= Model(model=pretrained_model,input_shape=(3,400,400),num_classes=number_of_classes,learning_rate=lr)\n","        wandb_logger = WandbLogger(project=\"classification-pytorch-lightning\",name=\"training_with_sweeps_models\",log_model=True)\n","        trainer = pl.Trainer(logger=wandb_logger, precision=16,max_epochs=5,\n","                         accelerator=\"gpu\")\n","        trainer.fit(model, dm)\n","        trainer.test(model, dm)\n","\n","        wandb.finish()\n","\n","\n"]},{"cell_type":"code","source":["wandb.agent(sweep_id, train, count=100,project=\"classification-pytorch-lightning\")"],"metadata":{"id":"r6V2MRn7fGqt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **download and use of best model**"],"metadata":{"id":"VWEwNTezH22r"}},{"cell_type":"code","source":["\n","run = wandb.init()\n","artifact = run.use_artifact('mirokery/classification-pytorch-lightning/model-n6njivt0:v0', type='model')\n","artifact_dir = artifact.download()\n"],"metadata":{"id":"qeivVUu6fmGk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(artifact_dir)\n","ckpt_path = os.path.join(artifact_dir,'model.ckpt')\n","model = Model.load_from_checkpoint(ckpt_path)\n","model.eval()\n","\n","class_to_idx ={'para': 0, 'spino': 1, 'stego': 2, 'trex': 3, 'velo': 4}\n","transform = transforms.Compose([\n","    transforms.Resize((400,400)),  # Resize to a fixed size\n","    transforms.ToTensor(),  # Convert image to tensor\n","     transforms.Normalize(mean=[0.4131, 0.3576, 0.2830], std=[0.2309, 0.2194, 0.2037])  # Normalize image\n","])\n","\n","image = Image.open(\"/content/drive/MyDrive/Test Images/test/DSC04764.JPG\").convert('RGB')\n","input_tensor = transform(image)\n","\n","input_batch = input_tensor.unsqueeze(0)\n","input_batch = input_batch.type(torch.cuda.FloatTensor)\n","print(input_batch.shape)\n","with torch.no_grad():\n","        output = model(input_batch)\n","\n","\n","print(output.shape)\n","print({i for i in class_to_idx if class_to_idx[i]==int(torch.argmax(output))})"],"metadata":{"id":"5HaU_ZhwgBTq"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"V100","mount_file_id":"1W8ITS4lz44EibDlCJkAWFQvcC-9bNcWT","authorship_tag":"ABX9TyNdhO8ZM6kGJDm7hEv86GFe"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}